#Depricated
kubectl run nginx --image=nginx

kubectl run mypod --image=nginx --restart=Never

kubectl get pods -o wide

kubectl run redis --image=redis --dry-run=client -o yaml > pod.yaml

kubectl apply -f pod.yaml

kubectl edit pod redis

#replication controller

kubectl create -f rc-controller.yml

kubectl get replicationcontroller

#replica set

kubectl create -f replicaset-definition.yml

kubectl get replicaset

kubectl delete replicaset myapp-replicaset
#above cmd also deletes all underlying pods


#scale

kubectl scale deploy app --replicas=2

kubectl replace -f rc-definition.yml

kubectl scale --replacas=6 -f rc-definition.yml

kubectl scale --replicas=6 replicaset myapp-replicaset


#Time saving tips

Create an NGINX Pod

kubectl run nginx --image=nginx

Generate POD Manifest YAML file (-o yaml). Don't create it(--dry-run)

kubectl run nginx --image=nginx --dry-run=client -o yaml

Create a deployment

kubectl create deployment --image=nginx nginx

kubectl expose deployment nginx --port 80

kubectl edit deployment nginx

kubectl scale deployment nginx --replicas=6

kubectl set image deployment nginx nginx=nginx:1.18

kubectl create -f def.yml
kubectl replace -f def.yml
kubectl delete -f def.yml
kubectl replace --force -f nginx.yml

Generate Deployment YAML file (-o yaml). Don't create it(--dry-run)

kubectl create deployment --image=nginx nginx --dry-run=client -o yaml

Generate Deployment YAML file (-o yaml). Don't create it(--dry-run) with 4 Replicas (--replicas=4)

kubectl create deployment --image=nginx nginx --dry-run=client -o yaml > nginx-deployment.yaml

Save it to a file, make necessary changes to the file (for example, adding more replicas) and then create the deployment.

kubectl create deployment does not have a --replicas option. You could first create it and then scale it using the kubectl scale command.




#namespace

kubectl create -f create-namespace-def.yml

kubectl create namespace dev

kubectl get pods --namespace=dev

#switch to a namespace permanently
kubectl config set-context $(kubectl config current-context) --namespace=dev

kubectl get pods --all-namespaces


#service

service type -->

NodePort, ClusterIP, LoadBalancer
clusterIP is default type of service spec type

NodePort Range: 30000 - 32767

kubectl create -f service-def.yml

kubectl get services

kubectl describe service servicename

service loadbalancing across multiple pods: random
session affinity : yes

kubectl expose pod redis --port=6379 --name=redis-service

Create a Service named redis-service of type ClusterIP to expose pod redis on port 6379

kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml

(This will automatically use the pod's labels as selectors)
Or
kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml  
(This will not use the pods labels as selectors, instead it will assume selectors as app=redis. 
You cannot pass in selectors as an option. So it does not work very well if your pod has a different label set. 
So generate the file and modify the selectors before creating the service)

#selectors and labels
kubectl get pods --selector env=dev

kubectl get all --selector env=prod
kubectl get all --selector env=prod,bu=finance,tier=frontend

#taint & toleration

taint is placed on Node, toleration is placed on pods

kubectl taint nodes node-name key=value:taint-effect
taint-effect : NoSchedule | PreferNoSchedule |  NoExecute

kubectl taint nodes node1 app=blue:NoSchedule

#untaint
kubectl taint nodes controlplane node-role.kubernetes.io/master:NoSchedule-

# by default master node has a taint. so scheduler does not place pod on the master node.
kubectl describe node kubemaster | grep Taint

#label nodes
kubectl label nodes node1 size=large

kubectl get nodes node01 --show-labels

use nodeSelector on pod definition

#node affinity 
Type of Affinity

requiredDuringSchedulingIgnoredDuringExecution

preferredDuringSchedulingIgnoredDuringExecution

Edit a POD

Remember, you CANNOT edit specifications of an existing POD other than the below.

    spec.containers[*].image

    spec.initContainers[*].image

    spec.activeDeadlineSeconds

    spec.tolerations

For example you cannot edit the environment variables, service accounts, resource limits
 (all of which we will discuss later) of a running pod. But if you really want to, you have 2 options:

1.
kubectl get pod webapp -o yaml > my-new-pod.yaml

2. kubectl edit pod pod-name

Edit Deployments

With Deployments you can easily edit any field/property of the POD template. Since the pod template is
a child of the deployment specification,  
every change the deployment will automatically delete and create a new pod with the new changes. 
So if you are asked to edit a property of a POD part of a deployment you may do that simply by running the command

kubectl edit deployment my-deployment 

#Resource Limit
RangeLimit object can be created
Pod can have resource limit

The status 'OOMKilled' indicates that the pod ran out of memory. 

#daemonset

kubectl get daemonset --all-namespaces

kubectl describe daemonset kube-proxy -n kube-system

We can create a deployment using imperative cmd and save the output to yaml file and then change the kind to DaemonSet
Delete the extra fields from the defnition such as replicas

#Static pod
location of static pod def files
/etc/kubernetes/manifests

can be created Pods only, no deployment, replicaset

--pod-manifest-path=/etc/Kubernetes/manifests

kubelet.service file-->
--kubeconfig=/var/lib/kubelet/kubeconfig \\

kubeconfig.yaml -->
staticPodPath: /etc/kubernetes/manifests

#view static pods
docker ps

How many static pods exist in this cluster in all namespaces?
kubectl get pods --all-namespaces 
and look for those with -controlplane appended in the name

Run the command ps -aux | grep kubelet and identify the config file -
 --config=/var/lib/kubelet/config.yaml. Then checkin the config file for staticPodPath.

 kubectl run --restart=Never --image=busybox static-busybox --dry-run=client -o yaml --command -- sleep 1000 > /etc/kubernetes/manifests/static-busybox.yaml


 Simply edit the static pod definition file and save it. If that does not re-create the pod, run: 
 'kubectl run --restart=Never --image=busybox:1.28.4 static-busybox --dry-run=client -o yaml --command -- sleep 1000 > /etc/kubernetes/manifests/static-busybox.yaml'

 #scheduler
kube-scheduler.service
--scheduler-name=default-scheduler

my-custom-scheduler.service
--scheduler-name=my-custom-scheduler


if using kube adm

/etc/kubernetes/manifests/kube-scheduler.yaml

to create custom scheduler, we can copy this schduler and provide a name in the command specification
- --scheduler-name=my-custom-scheduler
- --lock-object-name=my-custom-schduler-name

During implementation I have found that you need to pass the following as well

- --port=10282
- --secure-port=0

on the pod definition,
#provide schdulerName in the spec
#For using the custom scheduler

schedulerName: my-custom-scheduler

 kubectl get events

 kubectl logs my-custom-scheduler --name-space=kube-system

#monitoring and logging
#metrics server

minikube addons enable metrics-server

git clone https://github.com/kubernetes-incubator/metrics-serve
https://github.com/kodekloudhub/kubernetes-metrics-server.git

git clone https://github.com/kubernetes-incubator/metrics-server.git

kubectl create -f /deploy/1.8/

#show cpu usgae
kubectl top node

#show memory usage
kubectl top pod

#if pod contains one container
kubectl logs -f my-pod-name

#multiple container within a pod
kubectl logs -f my-pod-name container-name

#application lifecycle mgmt
#rolling update

kubectl apply -f deployment-def.yaml
kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1

kubectl set image deployment/nginx-deploy nginx=nginx:1.17 --record

Strategies: Recreate & RollingUpdate

kubectl describe deployment myapp-deployment
strategyType: Recreate
scaled down to zero and then scale upto n

strategyType: RollingUpdate
scaled down one at a time and scaling up new container

deployement happens using replicasets

kubectl get replicasets

kubectl rollout status deployment/myapp-deployment
kubectl rollout undo deployment/myapp-deployment
kubectl rollout history deployment/myapp-deployment

kubectl run cmd creates a deployment

# docker cmds

within dockerfile
CMD["sleep", "5"]

docker run ubuntu-sleeper  <-- would execute cmd sleep 5

docker run ubuntu-sleeper sleep 10  <-- over rides the entire cmd with the provided cmd, sleep 10
----------
ENTRYPOINT["sleep"]

docker run ubuntu-sleeper 10     <-- have to provide 10 as arg.
--------
ENTRYPOINT["sleep"]
CMD["5"]

docker run ubuntu-sleeper    <-- takes 5 as default arg
docker run ubuntu-sleeper 10    <-- overrides the default arg 5 and uses 10
------
#to override the entrypoint cmd with sleep2.0 instead of sleep
docker run --entrypoint sleep2.0 ubuntu-sleeper 10

#environment variables
docker run -e APP_COLOR=pink simple-webapp-color

#check cmd-arguments dir


#config map
kubectl create configmap \
    app-config --from-literal=APP_COLOR=blue
               --from-literal=APP_MOD=prod

kubectl create cm app-config --from-literal=APP_COLOR=blue

kubectl create configmap \
        app-config --from-file=app_config.properties

kubectl create -f config-map.yaml

kubectl get configmaps
kubectl describe configmaps

kubectl explain pods --recursive |  grep envFrom -A3

kubectl explain pods --recursive | less
#secret

kubectl create secret generic app-secret --from-literal=DB_HOST=mysql \
                                        --from-literal=DB_USER=root \
                                        --from-literal=DB_PASSWORD=passwd
                                
kubectl create secret generic \
            app-secret --from-file=app_secret.properties    

#hash a plain text into encoded string using base64
echo -n 'mysql' | base64

#decode
echo -n 'mysql' | base64 --decode

kubectl get secrets
kubectl describe secrets
kubectl get secret app-secret -o yaml

#volume - secret
secretName: app-secret #all secrets would be created as files

That is a task that will be run only  one time when the pod is first created. 
Or a process that waits  for an external service or database to be up before the actual application starts. 
That's where initContainers comes in.

An initContainer is configured in a pod like all other containers, 
except that it is specified inside a initContainers section

You can configure multiple such initContainers as well, 
like how we did for multi-pod containers. 
In that case each init container is run one at a time in sequential order.

If any of the initContainers fail to complete, 
Kubernetes restarts the Pod repeatedly until the Init Container succeeds.

kubectl -n elastic-stack exec -it app cat /log/app.log

kubectl get pods,svc

#cluster maintenace
pod-eviction-timeout=5m0s  #5 minutes
kube-controller-manager pod-eviction-timeout=5m0s ...

kubectl drain node1

#make node avaialable again after maintenance
kubectl uncordon node1

#make node unschedulable
kubectl cordon node2

#kubernetes software version
v1.13.4

major.minor.patch

#all these would be v1.13.4
kube-api-server
controller-manager
kube-scheduler
kubelet
kube-proxy
kubectl

#could be diff version
ETCD cluster, CoreDNS

            kube-api-server (x) 
controller-manager          kube-scheduler
    (x-1)                          (x-1)

    kubelet             kube-proxy
    (x-2)                   (x-2)

    
kubectl (x+1 > x-1)


#Upgrade components:

kubeadm uprade plan

kubeadm upgrade apply

##Upgrade process
#kubeadm upgrade

#upgrade master node
kubectl drain master/controlplane --ignore-daemonsets

apt-get upgrade -y kubeadm=1.12.0-00
kubeadm upgrade apply
apt-get upgrade -y kubelet=1.12.0-00
systemctl restart kubelet
kubectl get nodes

#upgrade worker nodes

kubectl drain node1
(kubectl drain node01 --ignore-daemonsets)
#if there are pods on the node which are not part of deployments/replicasets, the node needs to be force drained
#the pod would be lost forever in that process
kubectl drain node02 --ignore-daemonsets --force

apt-get upgrade -y kubeadm=1.12.0-00
apt-get upgrade -y kubelet=1.12.0-00
kubeadm upgrade node config --kubelet-version v1.12.0
systemctl restart kubelet

kubectl uncordon node1

#master
apt update
kubectl drain controlplane --ignore-daemonsets
apt install kubeadm=1.18.0-00
kubeadm version
kubeadm upgrade apply v1.18.0
apt install kubelet=1.18.0-00
systemctl restart kubelet
kubectl uncordon controlplane

#ssh node01
kubectl drain node01 --ignore-daemonsets
apt update
apt install kubeadm=1.18.0-00
kubeadm upgrade node
apt install kubelet=1.18.0-00
systemctl restart kubelet
kubectl uncordon node1


#back up

https://github.com/etcd-io/etcd/releases/download/v3.4.14/etcd-v3.4.14-linux-amd64.tar.gz

kubectl get all --all-namespaces -o yaml > all-deploy-services.yaml


restore--
linux academy

sudo ETCDCTL_API=3 etcdctl snapshot restore /home/cloud_user/etcd_backup.db \
--initial-cluster etcd-restore=https://etcd1:2380 \
--initial-advertise-peer-urls https://etcd1:2380 \
--name etcd-restore \
--data-dir /var/lib/etcd

etcd.service
--data-dir=var/lib/etcd

#snapshot etcd

ETCDCTL_API=3 etcdctl snapshot save snapshot.db
ETCDCTL_API=3 etcdctl snapshot status snapshot.db

etcdctl snapshot save -h and keep a note of the mandatory global options.

Since our ETCD database is TLS-Enabled, the following options are mandatory:

--cacert                                                verify certificates of TLS-enabled secure servers using this CA bundle

--cert                                                    identify secure client using this TLS certificate file

--endpoints=[127.0.0.1:2379]          This is the default as ETCD is running on master node and exposed on localhost 2379.

--key                                                      identify secure client using this TLS key file

use the help option for snapshot restore to see all available options for restoring the backup.

etcdctl snapshot restore -h

/etc/kubernetes/pki/etcd/server.key

--peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt
--peer-key-file=/etc/kubernetes/pki/etcd/peer.key
--peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt
--key-file=/etc/kubernetes/pki/etcd/server.key
--cert-file=/etc/kubernetes/pki/etcd/server.crt

export ETCDCTL_API=3

#restore etcd snapshot

service kube-apiserver stop

#backup cluster
kubectl logs etcd-controlplane -n kube-system

#backup the etcd cluster data

ETCDCTL_API=3 etcdctl snapshot save \
--endpoints=https://[127.0.0.1]:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
/opt/snapshot-pre-boot.db

#restore etcd  from snapshot

ETCDCTL_API=3 etcdctl snapshot restore \
--endpoints=https://[127.0.0.1]:2379 \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
--data-dir="/var/lib/etcd-from-backup" \
--initial-cluster="master=https://127.0.0.1:2380"
--name="master" \
--initial-advertise-peer-urls="https://127.0.0.1:2380" \
--initial-cluster-token="etcd-cluster-1"
 /opt/snapshot-pre-boot.db
 
//during hands-on lab 
ETCDCTL_API=3 etcdctl snapshot restore \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
--endpoints=127.0.0.1:2379 \
--data-dir="/var/lib/etcd-from-backup" \
--initial-advertise-peer-urls="https://127.0.0.1:2380" \
--initial-cluster-token="etcd-cluster-1" \
--name="controlplane" \
--initial-cluster="controlplane=https://127.0.0.1:2380" \
/opt/snapshot-pre-boot.db




ETCDCTL_API=3 etcdctl member list \
--cacert=/etc/kubernetes/pki/etcd/ca.crt \
--cert=/etc/kubernetes/pki/etcd/server.crt \
--key=/etc/kubernetes/pki/etcd/server.key \
--endpoints=127.0.0.1:2379

ETCDCTL_API=3 etcdctl snapshot save -h
ETCDCTL_API=3 etcdctl sanshot restore -h

Update ETCD POD to use the new hostPath directory 
/var/lib/etcd-from-backup by modifying the pod definition file at 
/etc/kubernetes/manifests/etcd.yaml. When this file is updated, 
the ETCD pod is automatically re-created as this is a static pod placed under 
the /etc/kubernetes/manifests directory.


  volumes:
  - hostPath:
      path: /var/lib/etcd-from-backup
      type: DirectoryOrCreate
    name: etcd-data
  - hostPath:
      path: /etc/kubernetes/pki/etcd
      type: DirectoryOrCreate
    name: etcd-certs

Note: as the ETCD pod has changed it will automatically restart, and also kube-controller-manager and 
kube-scheduler. Wait 1-2 to mins for this pods to restart. 
You can make a watch "docker ps | grep etcd" to see when the ETCD pod is restarted.

Note2: If the etcd pod is not getting Ready 1/1, then restart it by 
kubectl delete pod -n kube-system etcd-controlplane 
and wait 1 minute.

https://github.com/mmumshad/kubernetes-the-hard-way/blob/master/practice-questions-answers/cluster-maintenance/backup-etcd/etcd-backup-and-restore.md

#Security certificate details

openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout

#inspect service logs
journalctl -u etcd.service -l

kubectl logs etcd-master

docker logs container-name

#certificates api

#create a private key
openssl genrsa -out jane.key 2048

#create a csr
openssl req-new-key jane.key -subj "/CN=jane" -out jane.csr

create jane.csr.yaml using jane.csr in base64 format

cat jane.csr | base64

kubectl get csr

kubectl certificte approve jane
kubectl certificate deny jane
kubectl delete csr jane

#inspect the csr
kubectl get csr jane -o yaml

#to get the certiicate use the status certificate section of the yaml output
# the output is base64 encoded
kubectl get csr jane -o yaml

#decode base64 format
echo "KASK23KK2..." | base64 --decode

cat john.csr | base64 | tr -d "\n"

#within controller manager there is CSR-APPROVING CSR-SIGNING components

# To find the ca certificate look the kube controller manager definition
# look for --cluster-signing-cert-file and --cluster-signing-key-file
cat /etc/kubernetes/manifests/kube-controller-manager.yaml

#kubeconfig

curl https://my-kube-playground:6443/api/v1/pods \
    --key admin.key
    --cert admin.crt
    --cacert ca.crt

kubectl get pods \
    --server my-kube-playground:6443
    --client-key admin.key
    --client-certificate admin.crt
    --certificate-authority ca.crt

kubectl get pods --kubeconfig config

#Default location of kubeconfig file
$HOME/.kube/config

#clusters, context, users

kubectl config view
kubectl config view --kubeconfig=my-custom-config

kubectl config use-context prod-user@production
kubectl config -h

#namespace can be mentioned within a context

kubectl config --kubeconfig=/root/my-kube-config use-context reseacrh

#api

curl https://kube-master:6443/version

curl https://kube-master:6443/api/v1/pods

/metrics /healthz /apis /logs 

            /api                /apis

    coregroup                   namedgroup

#see api list
curl https://kube-master:6443/ -k
curl https://kube-master:6443/ -k | grep "name"

curl https://kube-master:6443/ -k \
    --key admin.key
    --cert admin.crt
    --cacert ca.crt
#The below cmd will read from default kubeconfig file assign cert, key and ca files
# so we dont nee to supply them when we run cmds afterwards
kubectl proxy


#kubectl proxy and kube proxy are different
#

#Authorozation modes

AllwaysAllow NODE ABAC RBAC WEBHOOK AlwaysDeny

we can supply the authorization mode at kube-apiserver startup cmd using comma seperated list for multiple modes

#Role based acces control #rbac

Roles and RoleBindings are namespace specific. By default, they are created in 'default' namespace
We can provide namespace in metadata section of RoleBinding

#View roles
kubectl get roles

kubectl get roles --all-namespaces
kubectl describe role developer

kubectl get rolebindings
kubectl describe rolebinding devuser-devrole-binding

#check access

kubectl auth can-i create deployments
kubectl auth can-i delete nodes

kubectl auth can-i create deployments --as dev-user
kubectl auth can-i delete nodes --as dev-user
kubectl get pods --as dev-user

kubectl create rolebinding dev-user-deployment --role=deployment-role --user=dev-user --namespace=blue \
--dry-run=client -o yaml > dev-user-deployment-binding.yaml

kubectl create role deployment-role --verb=create --resource=deployment --namespace=blue \
--dry-run=client -o yaml > deployment-role.yaml

kubectl auth can-i create pods --as dev-user --namespace test

kubectl api-resources --namespaced=true

kubectl api-resources --namespaced=false

#cluster role
clusterrole & clusterrolebinding

kubectl get clusterrole --no-headers | wc -l
kubectl get clusterrolebindings --no-headers | wc -l

kubectl create clusterrole storage-admin --resource=persistentvolumes,storageclasses --verb=list \
--dry-run=client -o yaml > storage-admin.yaml

kubectl create clusterrolebinding michelle-storage-admin --clusterrole=storage-admin --user=michelle

kubectl create clusterrolebinding pod-viewer \
  --clusterrole=pod-viewer \
  --serviceaccount=default:pod-viewer

The correct command is:
kubectl auth can-i <verb> <resource> --as=system:serviceaccount:<namespace>:<serviceaccountname> [-n <namespace>]

To check whether the tiller account has the right to create a ServiceMonitor object:
kubectl auth can-i create servicemonitor --as=system:serviceaccount:staging:tiller -n staging

Verifying if you Have Access
If you are not sure if your (or another) user is allowed to do a certain action, you can verify that with the kubectl auth can-i command.

$ kubectl auth can-i create deployments \
  --namespace production
yes
You can also impersonate users to check their access:

$ kubectl auth can-i create deployments \
  --namespace production \
  --as jane
no
or impersonate a Service Account to check if it is set up right:

$ kubectl auth can-i use podsecuritypolicies/privileged \
  -n kube-system \
  --as=system:serviceaccount:kube-system:calico-node
yes
You can also verify access for a whole group:

$ kubectl auth can-i get pods --as-group=system:masters
yes


#image

image: registry/user account/image repo
image: docker.io/nginx/nginx

docker login private-registry.io

docer run private-registry.io/apps/internal-app

kubectl create secret docker-registry regcred \
    --docker-server=  \
    --docker-username=
    --docker-password=
    --docker-email=


kubectl exec ubuntu-sleeper -- whoami

#solutions that support network policies:
Kube-router, Calico, Romana, Weave-net

#Solutions that do not support network policy
flannel

#To Work with Diff networking solutions
#Container Network Interface was created

kubectl get networkpolicy

kubectl describe networkpolicy 

#volume & storage

storage Driver
Volume Driver

docker volume create data_volume

/var/lib/docker
    volumes
        data_volume

#volume mounting
docker run -v data_volume:/var/lib/mysql mysql

#bind mounting
docker run -v /data/mysql:/var/lib/mysql mysql

docker run \
    --mount type=bind,source=/data/sql,target=/var/lib/mysql mysql

#storage drivers
AUFS, ZFS, BTRFS, Overlay, Overlay2, Device Mapper

#volume plugin
Local, Azure File Storage, VMWare vSphere Storage, GlusterFS, RexRay

storage driver selection depends on OS. Docker chosses storage driver automatically depending on OS.

#Container Storage Interface

#AWS EBS
docker run -it \
    --name mysql
    --volume-driver rexray/ebs
    --mount src=ebs-vol, target=/var/lib/mysql
    mysql

#Container Run Time Interface
#Support multiple container run times e.g. docker, rkt, cri-o

#Persistent Volume, PVC

kubectl get persistentvolumeclaim
kubectl get pv
kubectl get pvc

kubectl delete persistentvolumeclaim myclaim

#If there is mismatch of accessModes between persistent volume claim and persistent volume
#The claim will not bind to the PV

#If a PVC is being used by a Pod, if you try to delete the PVC,
#It gets stuck in terminating state until you delete the Pod

kubectl get sc


#Network

#Network Namespace

ip netns add red
ip netns add blue

ip netns
#list loopback interfaces
ip link

ip netns exec red ip link
ip -n red link

arp
ip netns exec red arp

route
ip netns exec red route


#run a bash shell inside a container

kubectl exec -it busybox -- bash
kubectl exec -it busybox -- sh

#ip route
ip r

kube-api-server --service-cluster-ip-range ipNet (default 10.0.0.0/24)

ps -aux | grep kube-api-server

iptable -L -t net | grep db-service

cat /var/log/kube-proxy.log

#get daemonset
kubectl -n kube-system get ds

#CoreDNS

cat /etc/coredns/Corefile

kubectl get configmap -n kube-system
coredns

kubectl get deployments -n kube-system
coredns

cat /etc/resolv.conf

#show mac addr
ip link show ens3

#from master node to check mac addr of node01
arp node01

#default gateway
ip route show default

#show port #
#which port kubescheduler listening to
netstat -nplt

#check live connection with port
#check which port is being used more for etcd
netstat -anp | grep etcd

That's because 2379 is the port of ETCD to which all control plane components connect to. 
2380 is only for etcd peer-to-peer connectivity. When you have multiple master nodes. In this case we don't.

#CNI

#CNI supported network plugins
ls /opt/cni/bin

#which kubelet uses ntwork plugin

ps -aux | grep kublet | grep network-plugin
#CNI configuration

/etc/cni/net.d

#binary executable file will be run by kubelet after a container and its associated namespace are created.
type: weave-net
cat /etc/cni/net.d/10-weave.conflist


kubectl run test --image=busybox --command -- sleep 4500
kubectl exec test -- ip route

#IP Range configured for the services within the cluster?
cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep cluster-ip-range

#type of proxy is the kube-proxy configured to use?
check logs of kube-proxy pod

#Identify DNS solutions
#check pods running
kubectl get pods -n kube-system

#IP of the CoreDNS server that should be configured on PODs to resolve services?
kubectl describe svc kube-dns -n kube-system

root domain/zone configured for this kubernetes cluster?
kubectl describe configmap coredns -n kube-system

#Which namespace is the Ingress Resource deployed in?
kubectl get ingress --all-namespaces

#Deploy Ingress controller steps:

1. create a namespace
kubectl create namespace ingress-space
2. The NGINX Ingress Controller requires a ConfigMap object. Create a ConfigMap object in the ingress-space.
kubectl create configmap nginx-configuration -n ingress-space
3. The NGINX Ingress Controller requires a ServiceAccount. Create a ServiceAccount in the ingress-space.
kubectl create serviceaccount ingress-serviceaccount -n ingress-space
4. create roles and rolebindings
5. Deploy ingress controller (nginx ingress controller)
6. Create a service to make ingress available to external users


#install kubeadm
1. https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
2. kubeadm init
    systemctl enable docker.service

    kubeadm token create --print-join-command

3. https://www.weave.works/docs/net/latest/kubernetes/kube-addon/
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"


#High Availabilty

controller manager works in active - standby mode in HA setup

kube-controller-manager --leader-elect true [other options]
                        --leader-elect-lease-duration 15s
                        --leader-elect-renew-deadline 10s
                        --leader-elect-retry-period 2s


#Trouble shooting
#Node status not ready

kubectl describe node node01

#ssh into node
#within node01
journalctl -u kubelet

#ssh into node and start the service
#systemstl start kubectl

systemctl daemon-reload
systemctl restart kubelet



#kubelet conf file on the worker node is located at 
/etc/systemd/system/kubelet.service.d

#cluster info
kubectl cluster-info

#The first thing to check is if the service has a valid endpoint? Does it point to the kube-dbs/core-dns?
kubectl -n kube-system get ep kube-dns


#JSON Path
kubectl get pods -o json

kubectl get nodes -o=jsonpath='{.items[*].metadata.name}'

kubectl get nodes -o=jsonpath='{.items[*].metadata.name}{"\n"}{.items[*].status.capacity.cpu}'

#Range -Loops
kubectl get nodes -o=jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.status.capacity.cpu}{"\n"}{end}'

#custom columns
kubectl get nodes -o=custom-columns=NODE:.metadata.name ,CPU:.status.capacity.cpu

kubectl get nodes --sort-by=.metadata.name
kubectl get pv  --sort-by=.spec.capacity.storage > /opt/outputs/storage-capacity-sorted.txt
kubectl get pv  --sort-by=.spec.capacity.storage -o=custom-columns=NAME:.metadata.name,CAPACITY:.spec.capacity.storage

kubectl config view --kubeconfig=my-kube-config

kubectl config view --kubeconfig=my-kube-config -o jsonpath="{.contexts[?(@.context.user=='aws-user')].name}"

kubectl config view --kubeconfig=my-kube-config -o jsonpath="{.contexts[?(@.context.user=='aws-user')].name}"




##exam tips

kubectl create clusterrole pvviewer-role --resource=persistentvolumes --verb=list

kubectl create clusterrolebinding pbinding --clusterrole=prole --serviceaccount=default:pvviewer


nc -zvw 2 nginx 80


scp the data dir to master node

for cluster upgrade question you will ssh to a node and start kubelet service

wget -qO- POD_IP_ADDRESS:80

kubectl get nodes -o=custom-columns='TAINT:.spec.taints,NODE:.metadata.name'

sudo journalctl -u kubelet